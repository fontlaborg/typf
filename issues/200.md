# Issue 200: Proposed Performance Speedups for `typf` and the `orge` Backend (Revised)

## 1. Introduction

This document outlines a series of proposed performance optimizations for the `typf` rendering engine, with a special focus on the `typf-orge` CPU rasterizer backend.

The analysis is based on a review of the architecture, source code, and existing documentation (`README.md`, `ALLOCATION.md`). The goal is to identify concrete, actionable improvements to increase rendering throughput and reduce latency. This revised edition adds more specific implementation details and code snippets for each proposal.

## 2. General Architectural Recommendations

### 2.1. Caching Strategies

The project already employs caching for fonts and shaped text, which is excellent.

-   **Proposal:** Investigate if the glyph cache in `typf-core` can be shared across different backend instances within the same process. If two `TextRenderer` objects are created (e.g., one for `harfbuzz` and one for `orge`), they should ideally hit the same cache for a given glyph if the rasterization parameters are identical. This would involve ensuring the `GlyphKey` is backend-agnostic or includes a backend identifier if the renderings are not compatible.

### 2.2. Batch Processing Parallelism

The `TextRenderer::render_batch` method in the Python bindings is currently sequential. This is a missed opportunity for a significant, easy-to-implement speedup on modern hardware.

-   **Proposal:** Use the `rayon` crate (already a workspace dependency) to parallelize the main loop in `render_batch`. Each item in the batch is an independent unit of work.

-   **Implementation Detail:** The change would be in `python/src/lib.rs`. The `iter()` call on the PyList can be replaced with `par_iter()`, and `collect()` will need to handle a `PyResult` from within the parallel map operation.

    ```rust
    // In python/src/lib.rs, inside fn render_batch
    use rayon::prelude::*; // Add this use statement

    // ... function signature ...
    fn render_batch<'py>(
        &self,
        py: Python<'py>,
        items: &Bound<'py, PyAny>,
        format: Option<&str>,
        max_workers: Option<usize>, // This can now be wired into a rayon thread pool
    ) -> PyResult<PyObject> {
        let list = items.downcast::<PyList>()?;
        let render_format = parse_render_format(format)?;

        // The core change is here: use `par_iter` and collect the results.
        let results: Vec<PyObject> = list
            .par_iter()
            .map(|item_any| {
                // The logic for rendering a single item is wrapped in this closure.
                // This closure will be executed in parallel by rayon.
                // Note: PyO3 GIL handling is crucial here. The heavy lifting (rendering)
                // should release the GIL.
                let dict = item_any.downcast::<PyDict>()?;
                // ... (extract text, font, and options from dict as before) ...

                let text: String = text_value.extract()?;
                let font_ref: PyRef<'py, Font> = font_obj.extract()?;

                // The render_internal call is the expensive part.
                // It needs to be wrapped to handle Python's GIL correctly in a threaded context.
                let result = py.allow_threads(move || {
                    self.render_internal(py, &text, &font_ref, render_format, Some(&options_dict))
                })?;

                Ok(result)
            })
            .collect::<PyResult<Vec<PyObject>>>()?;

        let py_list: Bound<'py, PyList> = PyList::new_bound(py, results);
        Ok(py_list.into_any().into_py(py))
    }
    ```

## 3. `typf-orge` Backend Specific Speedups

### 3.1. SIMD Acceleration for Grayscale Downsampling

-   **Observation:** The `downsample_to_grayscale` function in `grayscale.rs` is a textbook case for SIMD optimization. It sums coverage in an NxN block using nested scalar loops, which underutilizes modern CPUs.
-   **Proposal:** Vectorize the inner loops using the `wide` crate for stable SIMD. We can process 16 or 32 bytes from the monochrome bitmap at a time instead of one.

-   **Detailed Implementation (`wide` crate):**
    The following function is a drop-in replacement that processes chunks of the bitmap using `u8x16` (a 16-byte SIMD vector). It calculates coverage for multiple output pixels in a single inner loop.

    ```rust
    // In backends/typf-orge/src/grayscale.rs
    // Add `use wide::u8x16;` to the top of the file.

    fn downsample_to_grayscale_simd(
        mono: &[u8],
        mono_width: usize,
        _mono_height: usize, // Used for bounds checks, but loop is on width
        out_width: usize,
        out_height: usize,
        level: GrayscaleLevel,
    ) -> Vec<u8> {
        let factor = level.factor();
        let max_coverage = level.samples_per_pixel() as u32;
        let normalization_factor = 255.0 / max_coverage as f32;

        let mut output = vec![0u8; out_width * out_height];

        for out_y in 0..out_height {
            let src_y_base = out_y * factor;
            let out_row_start = out_y * out_width;

            for out_x in 0..out_width {
                let src_x_base = out_x * factor;
                let mut coverage = 0u32;

                // This is the core logic: sum a factor x factor block.
                for dy in 0..factor {
                    let src_row_start = (src_y_base + dy) * mono_width;
                    let row = &mono[src_row_start..];

                    // Process a row within the block
                    let mut row_coverage: u32 = 0;
                    let mut dx = 0;

                    // SIMD part: process chunks of 16 bytes.
                    // This is most effective for factor >= 4.
                    while dx + 16 <= factor {
                        let chunk = u8x16::from(&row[src_x_base + dx..]);
                        row_coverage += chunk.reduce_add() as u32;
                        dx += 16;
                    }

                    // Scalar part: handle the remainder.
                    for i in dx..factor {
                        row_coverage += row[src_x_base + i] as u32;
                    }
                    coverage += row_coverage;
                }

                // Convert coverage to 0-255 alpha and store.
                let alpha = (coverage as f32 * normalization_factor).round() as u8;
                output[out_row_start + out_x] = alpha;
            }
        }
        output
    }
    ```

### 3.2. Optimize the Active Edge List Sort

-   **Observation:** Sorting the entire `active_edges` list via `sort_by_x()` on every scanline (`scan_line_mono`) is inefficient, even if the list is mostly sorted.
-   **Proposal:** Replace the full sort with a linear-time merge operation. The `active_edges` from the previous line are already sorted. The new edges for the current scanline can be sorted once and then merged into the active list.

-   **Detailed Implementation:**
    1.  In `scan_line_mono`, sort the `new_edges` from the edge table *once*.
    2.  Create a new `Vec` for the next set of active edges.
    3.  Use a two-pointer approach to merge the old `active_edges` and the `new_edges` into the new `Vec`.
    4.  Swap the new `Vec` with `self.active_edges`.

    ```rust
    // In backends/typf-orge/src/scan_converter.rs

    // New helper function
    fn merge_edges(
        old_active: &EdgeList,
        newly_activated: &EdgeList,
        next_active: &mut EdgeList,
    ) {
        next_active.clear();
        let mut old_idx = 0;
        let mut new_idx = 0;

        while old_idx < old_active.len() && new_idx < newly_activated.len() {
            if old_active[old_idx].x < newly_activated[new_idx].x {
                next_active.push(old_active[old_idx].clone());
                old_idx += 1;
            } else {
                next_active.push(newly_activated[new_idx].clone());
                new_idx += 1;
            }
        }

        // Append remaining edges
        next_active.extend_from_slice(&old_active[old_idx..]);
        next_active.extend_from_slice(&newly_activated[new_idx..]);
    }


    // Modified scan_line_mono function
    fn scan_line_mono(&mut self, y: i32, bitmap: &mut [u8]) {
        if y < 0 || y >= self.height as i32 {
            return;
        }

        // A temporary Vec to hold the merged result. Can be a reusable buffer
        // stored in the ScanConverter struct to avoid re-allocation.
        let mut next_active_edges = EdgeList::with_capacity(self.active_edges.capacity());

        // Get and sort the new edges for this scanline.
        let y_usize = y as usize;
        let mut new_edges = self.edge_table[y_usize].clone(); // Cloning for this example.
        new_edges.sort_by_x(); // Sort only the new edges.

        // Remove inactive edges from the old active list.
        self.active_edges.remove_inactive(y);

        // Merge the already-sorted active_edges with the newly sorted new_edges.
        Self::merge_edges(&self.active_edges, &new_edges, &mut next_active_edges);
        std::mem::swap(&mut self.active_edges, &mut next_active_edges);

        // --- The rest of the function remains the same ---
        // Fill spans based on fill rule (uses the now sorted self.active_edges)
        match self.fill_rule {
            FillRule::NonZeroWinding => self.fill_nonzero_winding(y, bitmap),
            FillRule::EvenOdd => self.fill_even_odd(y, bitmap),
        }

        // Step all edges to next scanline
        self.active_edges.step_all();
    }
    ```

### 3.3. Optimize the `fill_span` Loop

-   **Observation:** The `fill_span` function writes to the bitmap one byte at a time.
-   **Proposal:** Replace the loop with `slice::fill()`, which the compiler will optimize to a `memset`.

-   **Implementation Detail:**

    **Before:**
    ```rust
    fn fill_span(&self, x1: i32, x2: i32, y: i32, bitmap: &mut [u8]) {
        if y < 0 || y >= self.height as i32 { return; }
        let x_start = x1.max(0).min(self.width as i32) as usize;
        let x_end = x2.max(0).min(self.width as i32) as usize;
        let row_offset = y as usize * self.width;
        for x in x_start..x_end {
            bitmap[row_offset + x] = 1; // Black
        }
    }
    ```

    **After:**
    ```rust
    fn fill_span(&self, x1: i32, x2: i32, y: i32, bitmap: &mut [u8]) {
        if y < 0 || y >= self.height as i32 || x1 >= x2 { return; }
        let x_start = (x1 as usize).clamp(0, self.width);
        let x_end = (x2 as usize).clamp(0, self.width);
        let row_offset = y as usize * self.width;
        if let Some(span) = bitmap.get_mut(row_offset + x_start .. row_offset + x_end) {
            span.fill(1); // The compiler will optimize this to memset
        }
    }
    ```

### 3.4. Advanced Dropout Control

-   **Observation:** The current `DropoutMode` is an enum but only `Simple` is noted in the `README`. Simple dropout control can still produce inconsistent results for very thin stems.
-   **Proposal:** Implement a more robust "smart" dropout control that considers the context of a span before rendering it. This improves visual quality without a significant performance penalty. The goal is to ensure that single-pixel-wide vertical stems are rendered consistently.

-   **Implementation Detail:** This logic can be added to the `fill_span` function.
    ```rust
    // In scan_converter.rs
    fn fill_span_with_dropout(&self, x1: i32, x2: i32, y: i32, bitmap: &mut [u8]) {
        let mut x_start = x1;
        let mut x_end = x2;

        if self.dropout_mode == DropoutMode::Smart {
            // If the span is less than a pixel wide...
            if x_end - x_start < 1 {
                // ...and it's not adjacent to an already-drawn pixel...
                let prev_pixel_on = x_start > 0 && bitmap[y as usize * self.width + (x_start - 1) as usize] == 1;
                if !prev_pixel_on {
                    // ...force it to be at least one pixel wide to prevent dropouts.
                    x_end = x_start + 1;
                }
            }
        }
        // ... proceed with the original fill_span logic using the (potentially modified) x_start and x_end.
    }
    ```

## 4. Benchmarking and Profiling

(This section remains unchanged, as it is still highly relevant.)

## 5. Summary of Recommendations (Revised)

| Priority | Component | Proposed Change | Estimated Impact |
| --- | --- | --- | --- |
| **High** | `grayscale.rs` | Use SIMD for `downsample_to_grayscale`. | High (4x-8x speedup in this stage) |
| **High** | `scan_converter.rs` | Replace full sort of active edge list with a linear-time merge. | High (Reduces algorithmic complexity in a hot loop) |
| **Medium** | `python/src/lib.rs` | Parallelize `render_batch` with Rayon. | High (For batch workloads) |
| **Medium** | `scan_converter.rs` | Use `slice.fill()` in `fill_span`. | Medium (Easy win, relies on compiler optimization) |
| **Medium** | `scan_converter.rs` | Implement "smart" dropout control. | Quality (Improves visual consistency of thin stems) |
| **Low** | `typf-orge` | Consider `bumpalo` for edge allocations *if* profiling confirms bottleneck. | Medium (More complex change) |