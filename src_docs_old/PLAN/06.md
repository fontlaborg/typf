# Part 06: Performance & Optimization

## Overview

This document details TypF's comprehensive performance optimization strategy, including SIMD vectorization, multi-level caching, parallelization, memory pooling, and benchmarking infrastructure designed to achieve extreme performance.

## 6.1 SIMD Optimization Architecture

### SIMD Trait Abstraction

```rust
// typf-core/src/simd/mod.rs

use std::arch::{x86_64::*, aarch64::*};

pub trait SimdBackend: Send + Sync {
    fn name(&self) -> &'static str;
    fn blend_rgba(&self, src: &[u8], dst: &mut [u8], alpha: u8);
    fn composite_glyphs(&self, coverage: &[u8], color: Color, dst: &mut [u8]);
    fn calculate_coverage(&self, samples: &[f32]) -> u8;
    fn transform_points(&self, points: &mut [Point], matrix: &Matrix);
}

pub struct SimdSelector;

impl SimdSelector {
    pub fn best_backend() -> Box<dyn SimdBackend> {
        #[cfg(all(target_arch = "x86_64", typf_simd_avx2))]
        if is_x86_feature_detected!("avx2") {
            return Box::new(Avx2Backend);
        }

        #[cfg(all(target_arch = "x86_64", typf_simd_sse41))]
        if is_x86_feature_detected!("sse4.1") {
            return Box::new(Sse41Backend);
        }

        #[cfg(all(target_arch = "aarch64", typf_simd_neon))]
        {
            return Box::new(NeonBackend);
        }

        Box::new(ScalarBackend)
    }
}
```

### AVX2 Implementation

```rust
// typf-core/src/simd/avx2.rs

#[cfg(all(target_arch = "x86_64", typf_simd_avx2))]
pub struct Avx2Backend;

#[cfg(all(target_arch = "x86_64", typf_simd_avx2))]
impl SimdBackend for Avx2Backend {
    fn name(&self) -> &'static str { "AVX2" }

    fn blend_rgba(&self, src: &[u8], dst: &mut [u8], alpha: u8) {
        unsafe {
            let alpha_vec = _mm256_set1_epi16(alpha as i16);
            let inv_alpha_vec = _mm256_set1_epi16(255 - alpha as i16);

            let chunks = src.chunks_exact(32).zip(dst.chunks_exact_mut(32));
            for (src_chunk, dst_chunk) in chunks {
                // Load 8 RGBA pixels (32 bytes)
                let src_pixels = _mm256_loadu_si256(src_chunk.as_ptr() as *const __m256i);
                let dst_pixels = _mm256_loadu_si256(dst_chunk.as_ptr() as *const __m256i);

                // Unpack to 16-bit for blending
                let src_lo = _mm256_unpacklo_epi8(src_pixels, _mm256_setzero_si256());
                let src_hi = _mm256_unpackhi_epi8(src_pixels, _mm256_setzero_si256());
                let dst_lo = _mm256_unpacklo_epi8(dst_pixels, _mm256_setzero_si256());
                let dst_hi = _mm256_unpackhi_epi8(dst_pixels, _mm256_setzero_si256());

                // Blend: dst = (src * alpha + dst * (255 - alpha)) / 255
                let blend_lo = _mm256_add_epi16(
                    _mm256_mullo_epi16(src_lo, alpha_vec),
                    _mm256_mullo_epi16(dst_lo, inv_alpha_vec)
                );
                let blend_hi = _mm256_add_epi16(
                    _mm256_mullo_epi16(src_hi, alpha_vec),
                    _mm256_mullo_epi16(dst_hi, inv_alpha_vec)
                );

                // Divide by 255 (approximation: (x + 128) * 257 >> 16)
                let result_lo = _mm256_mulhi_epu16(
                    _mm256_add_epi16(blend_lo, _mm256_set1_epi16(128)),
                    _mm256_set1_epi16(257)
                );
                let result_hi = _mm256_mulhi_epu16(
                    _mm256_add_epi16(blend_hi, _mm256_set1_epi16(128)),
                    _mm256_set1_epi16(257)
                );

                // Pack back to bytes
                let result = _mm256_packus_epi16(result_lo, result_hi);
                _mm256_storeu_si256(dst_chunk.as_mut_ptr() as *mut __m256i, result);
            }
        }
    }

    fn composite_glyphs(&self, coverage: &[u8], color: Color, dst: &mut [u8]) {
        unsafe {
            let color_r = _mm256_set1_epi16(color.r as i16);
            let color_g = _mm256_set1_epi16(color.g as i16);
            let color_b = _mm256_set1_epi16(color.b as i16);
            let color_a = _mm256_set1_epi16(color.a as i16);

            for (cov_chunk, dst_chunk) in coverage.chunks_exact(8)
                .zip(dst.chunks_exact_mut(32))
            {
                // Load coverage values (8 bytes)
                let cov = _mm_loadl_epi64(cov_chunk.as_ptr() as *const __m128i);
                let cov_16 = _mm256_cvtepu8_epi16(cov);

                // Load destination pixels
                let dst_pixels = _mm256_loadu_si256(dst_chunk.as_ptr() as *const __m256i);

                // Apply coverage to color alpha
                let effective_alpha = _mm256_mullo_epi16(cov_16, color_a);
                let effective_alpha = _mm256_mulhi_epu16(effective_alpha, _mm256_set1_epi16(257));

                // Composite
                // ... (similar blending logic as above with effective_alpha)
            }
        }
    }
}
```

### NEON Implementation (ARM)

```rust
// typf-core/src/simd/neon.rs

#[cfg(all(target_arch = "aarch64", typf_simd_neon))]
pub struct NeonBackend;

#[cfg(all(target_arch = "aarch64", typf_simd_neon))]
impl SimdBackend for NeonBackend {
    fn name(&self) -> &'static str { "NEON" }

    fn blend_rgba(&self, src: &[u8], dst: &mut [u8], alpha: u8) {
        unsafe {
            use std::arch::aarch64::*;

            let alpha_vec = vdupq_n_u16(alpha as u16);
            let inv_alpha_vec = vdupq_n_u16(255 - alpha as u16);

            for (src_chunk, dst_chunk) in src.chunks_exact(16)
                .zip(dst.chunks_exact_mut(16))
            {
                // Load 4 RGBA pixels
                let src_pixels = vld1q_u8(src_chunk.as_ptr());
                let dst_pixels = vld1q_u8(dst_chunk.as_ptr());

                // Convert to 16-bit for processing
                let src_lo = vmovl_u8(vget_low_u8(src_pixels));
                let src_hi = vmovl_u8(vget_high_u8(src_pixels));
                let dst_lo = vmovl_u8(vget_low_u8(dst_pixels));
                let dst_hi = vmovl_u8(vget_high_u8(dst_pixels));

                // Blend
                let blend_lo = vmlaq_u16(
                    vmulq_u16(dst_lo, inv_alpha_vec),
                    src_lo,
                    alpha_vec
                );
                let blend_hi = vmlaq_u16(
                    vmulq_u16(dst_hi, inv_alpha_vec),
                    src_hi,
                    alpha_vec
                );

                // Divide by 255 and pack
                let result_lo = vshrn_n_u16(blend_lo, 8);
                let result_hi = vshrn_n_u16(blend_hi, 8);
                let result = vcombine_u8(result_lo, result_hi);

                vst1q_u8(dst_chunk.as_mut_ptr(), result);
            }
        }
    }
}
```

## 6.2 Multi-Level Cache Architecture

### Cache Hierarchy

```rust
// typf-core/src/cache/mod.rs

pub struct CacheSystem {
    // L1: Hot cache for most recently used items
    l1_glyph: Arc<DashMap<GlyphCacheKey, Arc<RasterizedGlyph>>>,
    l1_shaping: Arc<DashMap<ShapingCacheKey, Arc<ShapingResult>>>,

    // L2: Larger cache with LRU eviction
    l2_glyph: Arc<Mutex<LruCache<GlyphCacheKey, Arc<RasterizedGlyph>>>>,
    l2_shaping: Arc<Mutex<LruCache<ShapingCacheKey, Arc<ShapingResult>>>>,

    // L3: Persistent cache (optional disk cache)
    l3_store: Option<Arc<PersistentCache>>,

    // Statistics
    stats: Arc<CacheStats>,
}

impl CacheSystem {
    pub fn new(config: CacheConfig) -> Self {
        Self {
            l1_glyph: Arc::new(DashMap::with_capacity(config.l1_size)),
            l1_shaping: Arc::new(DashMap::with_capacity(config.l1_size)),
            l2_glyph: Arc::new(Mutex::new(LruCache::new(
                NonZeroUsize::new(config.l2_size).unwrap()
            ))),
            l2_shaping: Arc::new(Mutex::new(LruCache::new(
                NonZeroUsize::new(config.l2_size).unwrap()
            ))),
            l3_store: config.persistent_cache_path.map(|path| {
                Arc::new(PersistentCache::new(path))
            }),
            stats: Arc::new(CacheStats::default()),
        }
    }

    pub async fn get_glyph(&self, key: &GlyphCacheKey) -> Option<Arc<RasterizedGlyph>> {
        // Check L1
        if let Some(glyph) = self.l1_glyph.get(key) {
            self.stats.l1_hits.fetch_add(1, Ordering::Relaxed);
            return Some(glyph.clone());
        }

        // Check L2
        if let Some(glyph) = self.l2_glyph.lock().await.get(key) {
            self.stats.l2_hits.fetch_add(1, Ordering::Relaxed);
            // Promote to L1
            self.promote_to_l1_glyph(key.clone(), glyph.clone());
            return Some(glyph.clone());
        }

        // Check L3 (persistent cache)
        if let Some(ref l3) = self.l3_store {
            if let Ok(Some(glyph)) = l3.get_glyph(key).await {
                self.stats.l3_hits.fetch_add(1, Ordering::Relaxed);
                // Promote to L2 and L1
                self.l2_glyph.lock().await.put(key.clone(), glyph.clone());
                self.promote_to_l1_glyph(key.clone(), glyph.clone());
                return Some(glyph);
            }
        }

        self.stats.misses.fetch_add(1, Ordering::Relaxed);
        None
    }

    fn promote_to_l1_glyph(&self, key: GlyphCacheKey, value: Arc<RasterizedGlyph>) {
        // Evict from L1 if full
        if self.l1_glyph.len() >= self.l1_glyph.capacity() {
            // Simple random eviction for L1
            if let Some(entry) = self.l1_glyph.iter().next() {
                self.l1_glyph.remove(entry.key());
            }
        }
        self.l1_glyph.insert(key, value);
    }
}
```

### Cache Key Design

```rust
// typf-core/src/cache/keys.rs

use ordered_float::OrderedFloat;

/// Optimized cache key for glyph rasterization
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub struct GlyphCacheKey {
    // Primary key components (most selective first)
    pub glyph_id: u32,
    pub font_hash: u64,  // Fast hash of font data
    pub ppem: u16,       // Pixels per em (quantized size)

    // Secondary components
    pub subpixel_x: u8,  // Quantized to 1/4 pixel
    pub subpixel_y: u8,
    pub hinting: u8,     // Hinting mode enum as u8

    // Packed variations (if any)
    pub variations_hash: u32,
}

impl GlyphCacheKey {
    pub fn new(
        glyph_id: u32,
        font: &FontRef,
        size: f32,
        subpixel_pos: (f32, f32),
        hinting: HintingMode,
        variations: &[(Tag, f32)]
    ) -> Self {
        // Quantize size to reduce cache entries
        let ppem = (size * 4.0).round() as u16 / 4; // 0.25 pixel accuracy

        // Quantize subpixel position
        let subpixel_x = ((subpixel_pos.0.fract() * 4.0).round() as u8).min(3);
        let subpixel_y = ((subpixel_pos.1.fract() * 4.0).round() as u8).min(3);

        // Fast hash for font identification
        let font_hash = xxhash::xxh64(font.data(), 0);

        // Hash variations
        let variations_hash = if variations.is_empty() {
            0
        } else {
            let mut hasher = FxHasher::default();
            for (tag, value) in variations {
                hasher.write_u32(tag.0);
                hasher.write_u32(value.to_bits());
            }
            hasher.finish() as u32
        };

        Self {
            glyph_id,
            font_hash,
            ppem,
            subpixel_x,
            subpixel_y,
            hinting: hinting as u8,
            variations_hash,
        }
    }
}

/// Optimized cache key for shaping results
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub struct ShapingCacheKey {
    pub text_hash: u64,
    pub font_hash: u64,
    pub features_hash: u32,
    pub script: u32,
    pub language: u32,
    pub direction: u8,
    pub size_quantized: u16,
}
```

### Persistent Cache

```rust
// typf-core/src/cache/persistent.rs

use rocksdb::{DB, Options, WriteBatch};

pub struct PersistentCache {
    db: DB,
    max_size: u64,
    current_size: AtomicU64,
}

impl PersistentCache {
    pub fn new(path: impl AsRef<Path>) -> Result<Self> {
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.set_compression_type(rocksdb::DBCompressionType::Lz4);
        opts.set_max_open_files(256);

        // Use bloom filters for faster lookups
        let mut block_opts = rocksdb::BlockBasedOptions::default();
        block_opts.set_bloom_filter(10, false);
        opts.set_block_based_table_factory(&block_opts);

        let db = DB::open(&opts, path)?;

        Ok(Self {
            db,
            max_size: 100 * 1024 * 1024, // 100MB default
            current_size: AtomicU64::new(0),
        })
    }

    pub async fn get_glyph(&self, key: &GlyphCacheKey) -> Result<Option<Arc<RasterizedGlyph>>> {
        let key_bytes = bincode::serialize(key)?;

        match self.db.get(&key_bytes)? {
            Some(value) => {
                let glyph: RasterizedGlyph = bincode::deserialize(&value)?;
                Ok(Some(Arc::new(glyph)))
            }
            None => Ok(None),
        }
    }

    pub async fn put_glyph(&self, key: GlyphCacheKey, glyph: Arc<RasterizedGlyph>) -> Result<()> {
        let key_bytes = bincode::serialize(&key)?;
        let value_bytes = bincode::serialize(&*glyph)?;

        // Check size limit
        let new_size = value_bytes.len() as u64;
        if self.current_size.load(Ordering::Relaxed) + new_size > self.max_size {
            self.evict_oldest(new_size).await?;
        }

        self.db.put(&key_bytes, &value_bytes)?;
        self.current_size.fetch_add(new_size, Ordering::Relaxed);

        Ok(())
    }

    async fn evict_oldest(&self, needed_space: u64) -> Result<()> {
        // LRU eviction based on access time metadata
        // Implementation details...
        Ok(())
    }
}
```

## 6.3 Parallelization Strategies

### Parallel Text Processing

```rust
// typf-core/src/parallel/mod.rs

use rayon::prelude::*;

pub struct ParallelProcessor {
    thread_pool: Arc<ThreadPool>,
    chunk_size: usize,
}

impl ParallelProcessor {
    pub fn process_text_runs(
        &self,
        runs: Vec<TextRun>,
        shaper: Arc<dyn Shaper>,
        font: Arc<FontInstance>
    ) -> Vec<ShapingResult> {
        // Process runs in parallel if beneficial
        if runs.len() < 2 || self.total_text_length(&runs) < 1000 {
            // Sequential for small inputs
            runs.into_iter()
                .map(|run| shaper.shape(&run, &font))
                .collect()
        } else {
            // Parallel processing
            runs.par_iter()
                .map(|run| shaper.shape(run, &font))
                .collect()
        }
    }

    pub fn render_glyphs_parallel(
        &self,
        glyphs: &[ShapedGlyph],
        renderer: Arc<dyn Renderer>,
        canvas: &mut Canvas
    ) {
        // Divide canvas into tiles for parallel rendering
        let tile_size = 256;
        let tiles = self.divide_into_tiles(canvas.bounds(), tile_size);

        // Render tiles in parallel
        let rendered_tiles: Vec<_> = tiles
            .par_iter()
            .map(|tile| {
                let tile_glyphs = self.filter_glyphs_for_tile(glyphs, tile);
                let mut tile_canvas = Canvas::new(tile.width, tile.height);

                for glyph in tile_glyphs {
                    renderer.render_glyph(glyph, &mut tile_canvas);
                }

                (tile.clone(), tile_canvas)
            })
            .collect();

        // Composite tiles back to main canvas
        for (tile, tile_canvas) in rendered_tiles {
            canvas.composite_tile(&tile_canvas, tile.x, tile.y);
        }
    }
}
```

### Work Stealing Queue

```rust
// typf-core/src/parallel/queue.rs

use crossbeam::deque::{Injector, Stealer, Worker};

pub struct WorkStealingQueue<T: Send> {
    injector: Arc<Injector<T>>,
    workers: Vec<Worker<T>>,
    stealers: Vec<Stealer<T>>,
}

impl<T: Send + 'static> WorkStealingQueue<T> {
    pub fn new(num_workers: usize) -> Self {
        let injector = Arc::new(Injector::new());
        let mut workers = Vec::with_capacity(num_workers);
        let mut stealers = Vec::with_capacity(num_workers);

        for _ in 0..num_workers {
            let worker = Worker::new_fifo();
            stealers.push(worker.stealer());
            workers.push(worker);
        }

        Self {
            injector,
            workers,
            stealers,
        }
    }

    pub fn spawn_workers<F>(&self, f: F)
    where
        F: Fn(T) + Send + Sync + Clone + 'static,
    {
        let handles: Vec<_> = (0..self.workers.len())
            .map(|i| {
                let injector = self.injector.clone();
                let worker = self.workers[i].clone();
                let stealers = self.stealers.clone();
                let f = f.clone();

                std::thread::spawn(move || {
                    loop {
                        // Try local queue first
                        let task = worker.pop()
                            .or_else(|| {
                                // Try stealing from injector
                                std::iter::repeat_with(|| {
                                    injector.steal_batch_and_pop(&worker)
                                })
                                .find(|s| !s.is_retry())
                                .and_then(|s| s.success())
                            })
                            .or_else(|| {
                                // Try stealing from other workers
                                stealers.iter()
                                    .cycle()
                                    .find_map(|s| s.steal().success())
                            });

                        match task {
                            Some(t) => f(t),
                            None => {
                                // No work available
                                std::thread::yield_now();
                            }
                        }
                    }
                })
            })
            .collect();
    }
}
```

## 6.4 Memory Pool Architecture

### Object Pools

```rust
// typf-core/src/memory/pool.rs

pub struct ObjectPool<T> {
    pool: Arc<ArrayQueue<T>>,
    factory: Arc<dyn Fn() -> T + Send + Sync>,
    max_size: usize,
}

impl<T: Send + 'static> ObjectPool<T> {
    pub fn new<F>(max_size: usize, factory: F) -> Self
    where
        F: Fn() -> T + Send + Sync + 'static,
    {
        let pool = Arc::new(ArrayQueue::new(max_size));

        // Pre-populate pool
        for _ in 0..max_size / 2 {
            let _ = pool.push(factory());
        }

        Self {
            pool,
            factory: Arc::new(factory),
            max_size,
        }
    }

    pub fn get(&self) -> PooledObject<T> {
        let obj = self.pool.pop().unwrap_or_else(|| (self.factory)());

        PooledObject {
            value: Some(obj),
            pool: self.pool.clone(),
        }
    }
}

pub struct PooledObject<T> {
    value: Option<T>,
    pool: Arc<ArrayQueue<T>>,
}

impl<T> Deref for PooledObject<T> {
    type Target = T;

    fn deref(&self) -> &Self::Target {
        self.value.as_ref().unwrap()
    }
}

impl<T> Drop for PooledObject<T> {
    fn drop(&mut self) {
        if let Some(value) = self.value.take() {
            // Return to pool if there's space
            let _ = self.pool.push(value);
        }
    }
}
```

### Arena Allocator

```rust
// typf-core/src/memory/arena.rs

pub struct Arena {
    chunks: Vec<Vec<u8>>,
    current: Cell<usize>,
    offset: Cell<usize>,
    chunk_size: usize,
}

impl Arena {
    pub fn new(chunk_size: usize) -> Self {
        let mut chunks = Vec::new();
        chunks.push(Vec::with_capacity(chunk_size));

        Self {
            chunks,
            current: Cell::new(0),
            offset: Cell::new(0),
            chunk_size,
        }
    }

    pub fn alloc<T>(&self, value: T) -> &T {
        unsafe {
            let size = std::mem::size_of::<T>();
            let align = std::mem::align_of::<T>();

            let ptr = self.alloc_raw(size, align);
            std::ptr::write(ptr as *mut T, value);
            &*(ptr as *const T)
        }
    }

    unsafe fn alloc_raw(&self, size: usize, align: usize) -> *mut u8 {
        let offset = self.align_offset(self.offset.get(), align);

        if offset + size > self.chunk_size {
            // Allocate new chunk
            self.new_chunk();
            return self.alloc_raw(size, align);
        }

        let chunk = &self.chunks[self.current.get()];
        let ptr = chunk.as_ptr().add(offset) as *mut u8;
        self.offset.set(offset + size);

        ptr
    }

    fn new_chunk(&self) {
        // Allocate new chunk and update current
        // Implementation details...
    }

    fn align_offset(&self, offset: usize, align: usize) -> usize {
        (offset + align - 1) & !(align - 1)
    }
}
```

## 6.5 Benchmarking Infrastructure

### Benchmark Suite

```rust
// benches/typf_bench.rs

use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId, Throughput};

fn benchmark_shaping(c: &mut Criterion) {
    let mut group = c.benchmark_group("shaping");

    // Test data
    let texts = vec![
        ("latin_simple", "Hello, World!"),
        ("latin_complex", "ffi fl √¶≈ì WAVE"),
        ("arabic", "ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ"),
        ("cjk", "‰Ω†Â•Ω‰∏ñÁïå„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå"),
        ("emoji", "Hello üëã World üåç"),
        ("mixed", "Hello ŸÖÿ±ÿ≠ÿ®ÿß ‰Ω†Â•Ω üëã"),
    ];

    let font = load_test_font("NotoSans-Regular.ttf");

    for backend in &[ShapingBackend::HarfBuzz, ShapingBackend::IcuHarfBuzz] {
        let shaper = create_shaper(*backend);

        for (name, text) in &texts {
            group.throughput(Throughput::Bytes(text.len() as u64));

            group.bench_with_input(
                BenchmarkId::new(format!("{:?}", backend), name),
                text,
                |b, text| {
                    b.iter(|| {
                        shaper.shape(black_box(text), &font, &default_params())
                    })
                }
            );
        }
    }

    group.finish();
}

fn benchmark_rendering(c: &mut Criterion) {
    let mut group = c.benchmark_group("rendering");

    let shaped = create_test_shaped_text();
    let sizes = vec![12.0, 16.0, 24.0, 48.0, 96.0];

    for backend in &[RenderBackend::Orge, RenderBackend::Skia, RenderBackend::Zeno] {
        let renderer = create_renderer(*backend);

        for size in &sizes {
            group.throughput(Throughput::Elements(shaped.glyphs.len() as u64));

            group.bench_with_input(
                BenchmarkId::new(format!("{:?}", backend), size),
                size,
                |b, &size| {
                    let params = RenderParams {
                        size,
                        ..default_render_params()
                    };

                    b.iter(|| {
                        renderer.render(black_box(&shaped), black_box(&params))
                    })
                }
            );
        }
    }

    group.finish();
}

fn benchmark_cache(c: &mut Criterion) {
    let mut group = c.benchmark_group("cache");

    let cache = CacheSystem::new(CacheConfig {
        l1_size: 256,
        l2_size: 2048,
        persistent_cache_path: None,
    });

    // Pre-populate cache
    let keys: Vec<_> = (0..10000)
        .map(|i| GlyphCacheKey {
            glyph_id: i % 256,
            font_hash: 12345,
            ppem: (12 + i % 10) as u16,
            subpixel_x: (i % 4) as u8,
            subpixel_y: ((i / 4) % 4) as u8,
            hinting: 0,
            variations_hash: 0,
        })
        .collect();

    for key in &keys[..2048] {
        let glyph = create_test_glyph();
        cache.put_glyph(key.clone(), glyph);
    }

    group.bench_function("l1_hit", |b| {
        let key = &keys[0]; // Should be in L1
        b.iter(|| {
            cache.get_glyph(black_box(key))
        })
    });

    group.bench_function("l2_hit", |b| {
        let key = &keys[1000]; // Should be in L2
        b.iter(|| {
            cache.get_glyph(black_box(key))
        })
    });

    group.bench_function("miss", |b| {
        let key = &keys[5000]; // Not in cache
        b.iter(|| {
            cache.get_glyph(black_box(key))
        })
    });

    group.finish();
}

fn benchmark_simd(c: &mut Criterion) {
    let mut group = c.benchmark_group("simd");

    let src = vec![0u8; 4096];
    let mut dst = vec![0u8; 4096];

    let backends = vec![
        ("scalar", SimdSelector::scalar_backend()),
        ("auto", SimdSelector::best_backend()),
    ];

    #[cfg(all(target_arch = "x86_64", typf_simd_avx2))]
    if is_x86_feature_detected!("avx2") {
        backends.push(("avx2", Box::new(Avx2Backend)));
    }

    for (name, backend) in backends {
        group.bench_function(BenchmarkId::new("blend_rgba", name), |b| {
            b.iter(|| {
                backend.blend_rgba(
                    black_box(&src),
                    black_box(&mut dst),
                    black_box(128)
                )
            })
        });
    }

    group.finish();
}

criterion_group!(
    benches,
    benchmark_shaping,
    benchmark_rendering,
    benchmark_cache,
    benchmark_simd
);
criterion_main!(benches);
```

### Performance Profiling

```rust
// typf-core/src/profiling/mod.rs

#[cfg(feature = "profiling")]
pub use puffin;

#[cfg(not(feature = "profiling"))]
pub mod puffin {
    #[macro_export]
    macro_rules! profile_scope {
        ($name:expr) => {};
        ($name:expr, $data:expr) => {};
    }
}

pub struct Profiler;

impl Profiler {
    pub fn start_frame() {
        #[cfg(feature = "profiling")]
        puffin::GlobalProfiler::lock().new_frame();
    }

    pub fn end_frame() {
        #[cfg(feature = "profiling")]
        {
            let data = puffin::GlobalProfiler::lock().report();
            // Send to profiling server or save to file
        }
    }
}

// Usage in hot paths
impl Shaper for HarfBuzzShaper {
    fn shape(&self, text: &str, font: &FontRef, params: &ShapingParams) -> ShapingResult {
        profile_scope!("HarfBuzz::shape");

        {
            profile_scope!("cache_lookup");
            if let Some(cached) = self.cache.get(&cache_key) {
                return cached;
            }
        }

        {
            profile_scope!("harfbuzz_shape", format!("text_len={}", text.len()));
            // Actual shaping logic
        }
    }
}
```

## 6.6 Performance Targets

### Benchmark Targets

| Operation | Target | Measurement | Hardware |
|-----------|--------|-------------|----------|
| Simple Latin shaping | < 10¬µs | Per 100 chars | M1 MacBook |
| Complex Arabic shaping | < 50¬µs | Per 100 chars | M1 MacBook |
| Glyph rasterization | < 1¬µs | Per glyph @ 16px | M1 MacBook |
| L1 cache hit | < 50ns | Single lookup | Any |
| L2 cache hit | < 200ns | Single lookup | Any |
| RGBA blending (SIMD) | > 10GB/s | Throughput | x86_64 AVX2 |
| Parallel rendering | > 0.8x/core | Scaling efficiency | 8-core |

### Memory Targets

| Component | Target | Notes |
|-----------|--------|-------|
| Per-glyph overhead | < 100 bytes | Metadata only |
| Shaping result | < 10 bytes/char | Compressed format |
| L1 cache size | < 1MB | Hot data |
| L2 cache size | < 10MB | Working set |
| Peak memory (1M chars) | < 100MB | Full pipeline |

## 6.7 Optimization Guidelines

### Hot Path Optimizations

```rust
// typf-core/src/optimize.rs

/// Guidelines for hot path optimization
pub mod guidelines {
    /// 1. Avoid allocations in hot paths
    #[inline(always)]
    pub fn process_glyph_fast(glyph: &Glyph, scratch: &mut ScratchBuffer) {
        // Reuse scratch buffer instead of allocating
        scratch.clear();
        scratch.process(glyph);
    }

    /// 2. Use branch prediction hints
    #[inline]
    pub fn likely_path(condition: bool) -> bool {
        #[cfg(target_arch = "x86_64")]
        unsafe {
            std::intrinsics::likely(condition)
        }
        #[cfg(not(target_arch = "x86_64"))]
        condition
    }

    /// 3. Prefetch cache lines
    #[inline(always)]
    pub fn prefetch_read<T>(ptr: *const T) {
        #[cfg(target_arch = "x86_64")]
        unsafe {
            std::arch::x86_64::_mm_prefetch(
                ptr as *const i8,
                std::arch::x86_64::_MM_HINT_T0
            );
        }
    }

    /// 4. Use const generics for compile-time optimization
    pub fn blend_pixels<const ALPHA: u8>(src: &[u8], dst: &mut [u8]) {
        if ALPHA == 0 {
            return; // No-op
        } else if ALPHA == 255 {
            dst.copy_from_slice(src); // Simple copy
        } else {
            // Full blend
            simd_blend(src, dst, ALPHA);
        }
    }
}
```

## Next Steps

With performance optimization strategies defined, Part 07 will detail the CLI application and Python bindings implementation.
